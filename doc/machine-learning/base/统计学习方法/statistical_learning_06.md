# 逻辑斯谛回归和最大熵模型

[TOC]

逻辑斯地回归是经典分类方法，最大熵模型是概率学习模型的一个准则，将其推广到分类问题，得到最大熵模型。

## 6.1逻辑斯谛回归模型

### 逻辑斯地分布

X服从逻辑斯第分布的分布函数和概率函数
$$
F(x) = P(X<x) = \frac{1}{1 + e^{-\frac{x-\mu}{\gamma}}} \\
f(x) = F`(x) = \frac{e^{-\frac{x-\mu}{\gamma}}}{\gamma(e^{-\frac{x-\mu}{\gamma}})^2}
$$
$\mu$为位置参数，$\gamma$为形状参数

一个事件的几率是指发生的概率和不发生的概率的比值 p/(1-p)

输出Y=1的对数几率是输入x的线性函数的模型是逻辑斯地分布模型

## 6.2最大熵模型

最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。

定义联合分布P(X, Y)的经验分布，边缘分布P(X)的经验分布
$$
P^\overline(X=x, Y=y) = v(X=x, Y=y) / N
$$
v(X=x, Y=y) 表示训练样本中出现(x, y)的频数，N表示训练样本容量。

定义特征函数，用特征函数来描述输入x和输出y的一个事实。

$$
f(x, y) = \begin{cases} 1 & x和y满足某一事实 \\ 0 \end{cases}
$$

将 A（特征函数关于经验期望的分布值） 和 B（特征函数关于模型P(Y / X) 与P(X)经验分布的期望值）相等的条件，作为模型的约束条件，假设有n个特征函数，那么就有n个约束条件。

### 拉格朗日

这里主要涉及到了拉格朗日函数。拉格朗日乘数法（以数学家约瑟夫·拉格朗日命名）是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。这种方法可以将一个有n个变量与k个约束条件的最优化问题转换为一个解有n + k个变量的方程组的解的问题。这种方法中引入了一个或一组新的未知数，即拉格朗日乘数，又称拉格朗日乘子，或拉氏乘子，它们是在转换后的方程，即约束方程中作为梯度（gradient）的线性组合中各个向量的系数。
https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0

查看例6.2-P101

最大熵模型学习中，对偶函数极大化等价于最大熵模型的极大似然估计。

## 6.3模型学习的最优化算法

### 改进的迭代尺度算法IIS

