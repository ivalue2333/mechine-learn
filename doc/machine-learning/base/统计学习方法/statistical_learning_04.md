# 第四章：朴素贝叶斯

朴素贝叶斯(naive bayes)是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立的假设求出后验概率最大的对应的输出y。

[TOC]

## 4.1朴素贝叶斯的学习与分类

假设选择0-1损失函数

$$ L(Y, f(X)) = \begin{cases} 1 & Y != f(X) \\ 0 & Y = f(X) \end{cases}$$

经验风险最小化等价于后验概率最大化

> $$ f(x) = arg max_{c_k} P(c_k / X = x) $$

## 4.2朴素贝叶斯法的参数估计

### 极大似然估计

在朴素贝叶斯中，学习意味着估计如下两个
$$
先验概率的极大似然估计： P(Y=c_k) = \frac{\sum_{i=1}^{N} I(y_i = c_k)}{N} \\
设第 j 个特征 x^{(j)} 的取值范围是{a_{j1}...a_{js_j}} \\
则条件概率的极大似然估计：P(x^{(j) = a_{jl}} / Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} =a_{jl} / yi=c_k)}{ \sum_{i=1}^N I(y_i = c_k)}
$$
可以用极大似然估计法估计相关的概率。

### 学习与分类算法

### 贝叶斯估计

