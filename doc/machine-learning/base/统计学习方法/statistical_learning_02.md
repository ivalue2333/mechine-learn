# 第二章：感知机

[TOC]

## 感知机模型

感知机模型是二元分类的线性分类模型

>  f(x) = sign(w*x +b)

w,叫做权值或者权值向量(weight vector)

b,叫做偏置(bias)

sign,符号函数

> $sign(x) = \begin{cases} 1 & x>= 0 \\ -1 & x< 0 \end{cases}$

wx + b 是空间内的一个超平面

## 感知机学习策略

### 损失函数

损失函数是误分类点到超平面的距离之和S

任意一点到超平面的距离, 其中||w||表示L2范数

> $ \frac{1}{||w||}|wx_0 + b|$

对于误分类的(xi, yi)，有(分类正确了同号，分类错误，异号)

> $ - y_i (w_i x + b) > 0$

### 所有误分类点到超平面的总距离

> $$ \frac{1}{||w||} \sum_{x_i \in M}y_i(wx_i + b)$$

M表示误分类点的集合

## 感知机学习算法

### 损失函数梯度

> $ \nabla L(w, b) = -\sum_{x_i \in M} y_i x_i$
>
> $\nabla L(w, b) = - \sum_{x_i \in M} y_i$

随机选择一个(xi, yi)，对w， b更新，

$w = w + \eta y_i x_i$

$b = b + \eta y_i $

**eta是步长，又称为学习效率 **

- 原始形式
- 前提：损失函数 -sum(yi(w*xi + b))，其中xi属于误分类集合
  - 选取初值, w0, b0
  - 在训练集中选取数据， xi, yi
  - 如果yi(w*xi + b) <= 0（这意味着损失函数大于0，如果损失函数等于0，则意味着所有的点都分类正确，没有xi属于误分类集合）
    - w <-- w + lr * yi * xi （这取决于loss关于w的导数）
    - b <-- b + lr * yi  （这取决于loss关于b的导数）
    - lr即为学习效率
- 这种学习方法有一个形象的解释，当一个点被误分类了，修改w, b的值，使得分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直到超平面越过该分类点，使其被正确分类。

### 感知机的收敛

即误分类是有上界(bound)的，不是趋向于无穷。