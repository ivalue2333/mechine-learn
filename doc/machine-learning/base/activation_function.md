# 激活函数

[TOC]

## Synopsis

### feature

- 非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了
- 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。
- 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数
- f(x)≈x ： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值
- 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.

## Refers

cs231n激活函数：http://cs231n.github.io/neural-networks-1/

简单总结：https://juejin.im/entry/58a1576e2f301e006952ded1

简单总结：https://blog.csdn.net/cyh_24/article/details/50593400

## Details

### Sigmoid

- 目前不太受欢迎...

### tanh

### ReLU	



