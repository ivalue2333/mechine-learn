# 	loss

[TOC]

## Synopsis

## Refers

## Details

### 线性关系

> y = b + wx

- y：指的是预测标签（理想输出值）
- b： 指的是偏差（y 轴截距）。而在一些机器学习文档中，它称为w0
- w1：指的是特征 1 的权重。权重与上文中用m 表示的“斜率”的概念相同
- x1：特征（已知输入项）

### 损失的产生

简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为经验风险最小化。

损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。

> 训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。

### 损失函数

损失函数和代价函数是一個意思

#### 平方损失

#### 均方损失

### 降低损失

#### 迭代方法

> 在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。

> 通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。

>  回归问题产生的损失与权重图为凸形

#### 凸函数

凸函数延x轴方向的梯度（导数）是递增的

#### 梯度下降

> 梯度下降，gradient descent 是一个一阶最优化算法
>
> 梯度下降法寻找的是梯度的'0'值，这个位置也是损失的最小值，这个位置的权值就是解

- 梯度下降法的起点
  - 梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度等于导数。
- 梯度是一個矢量
  - 方向
  - 大小
- 梯度下降法依赖于负梯度
- 一个梯度步长将我们移动到损失曲线上的下一个点

##### 学习速率

梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

**超参数** 是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间

相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳

- 损失函数的梯度小，则步长稍微大一点
- 损失函数的梯度大，则步长稍微小一点

##### 随机梯度下降法

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。